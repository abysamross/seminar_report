\chapter{Background}

\section{Memory Management in Native Systems} \label{native}
Memory management in non-virtualized native multiprogramming system is achieved using Virtual
Memory. In multiprogrammed systems several programs are resident in memory at the same time.
The memory management policy in such a system deals with protecting the memory of one program from 
another, loading a program into available space in main memory, (de)allocating memory dynamically
from/to programs.\\
While programs are complied and linked with addresses starting at $0$ and CPU uses 
these addresses to access the binary, it isn't necessary (and is the not the case that) that a
program will get physical memory with the same addresses or it is not even guaranteed that there
will be enough space in the physical memory to load the entire binary of the program. Most of the
times only the immediately needed part of the program is loaded onto the available physical
memory, (the rest will be held in a backing store - often the hard disk) and they will share the
physical memory space along with parts of other programs. And the OS may choose to evict parts of
the program to the backing store when in need of memory as a part of its memory management policy.
Therefore the addresses generated by CPU needs to be translated into the corresponding physical
addresses. The address generated by CPU is called \textit{Virtual Address} or VA . Thus we need to
have a mechanism of \textit{Virtual Address (VA)} $\rightarrow$ \textit{Physical Address (PA)}
translation.\\
Hence the illusion that is given to a program that there is enough physical memory available to
store its entire binary combined with relocation of code having contiguous virtual addresses into
- not necessarily contiguous - available physical memory chunks are the central ideas of virtual
memory.\\
Virtual Memory can be implemented in more than one ways. Paging is one of them. The idea behind
paging is to divide the virtual address space and the physical memory into same sized units of
allocation called \textit{pages}. Hence the virtual address space is divided into \textit{virtual
pages} or simply \textit{pages} and physical memory into \textit{physical pages} or
\textit{frames} or \textit{physical/machine frames}. It is thus clear that not all pages of a
program will be present in the physical memory when the program is executing, the contiguous
virtual pages belonging to the same program needn't be allocated contiguous frames and it is
necessary to provide a way to map from the virtual pages to the associated machine frames.\\
This mapping should be there for each program and this mapping is called the \textit{page table}.
The organisation of page table is closely tied to the size of a page or a frame. Page size is
taken as \textit{power of 2} as this ensures that all binary representable addresses can be
utilized and address manipulation can be done without arithmetic operations. There should an be an
entry for all virtual page addresses of a program in its page table. This will result in a very
long (large) page table and all the page tables of all programs that are currently resident in
memory will consume considerable amount of physical memory. So instead of storing such a single
large page table per program in memory we break the page tables into different levels to have a
tree like structure. Thus following the design principles of virtual memory it is not necessary
that even all the levels of the page table will be resident in memory all the time.\\
Often the entries in each level are grouped into different sets. And each entry will contain a
physical address and some flag bits. The flag bits store permissions and other info about the this
entry and the physical address in the entry points to a set of entries in the next level. These
sets of entries in each level are often limited to single physical frame. Thus the physical
address in a page table entry points to a physical frame in the next level. There is an exception
for the last level of the page table. An entry in the last level contains the physical address of
the actual virtual page that we were looking for and the flag bits of the entry include
information like whether the physical page it points to is present or not. The physical address of
the root of the page table (i.e. the physical address of the base of the outermost level of page
table) is stored in a hardware register.\\
The translation of virtual address to physical address happens in the following manner:\\
The base address of the program's page table is obtained from the hardware register storing it. To
this base address we add that part of the virtual address that corresponds to the outermost level. 
Thus we get the corresponding entry in the outer most level which points to the base address of
the corresponding set of entries of the next level (i.e. a physical page of the next level) . Now
to get to the corresponding entry within that physical page we add the part of virtual address for
the next level to the physical page address we obtained from the outermost level. This process is
continued till we get to a last level entry from which we get the physical frame address 
corresponding to the virtual address. And to go to the exact physical address location
corresponding to the virtual address location add the last part of the virtual that doesn't
correspond to any page table level i.e. the offset part to the physical frame address.\\    
Hence it is clear from the above discussion that the parts of the virtual address that correspond
to each level of page table give us an offset within that level and last part of the virtual
address that doesn't correspond to any page table level gives us the offset within the actual
frame that holds this virtual address. Illustration in figure \ref{fig:pagetable}.\\
For a virtual address generated by the CPU the translation to physical address i.e. traversing the
levels of page table happens in hardware. If the corresponding physical page is present in the
translated location it is a \textit{hit} else it is a \textit{fault}. Page faults are to be
resolved by moving in the corresponding missing page from the backing store. Certain architectures
like x86 choose to cache recently accessed virtual addresses and their mappings in a small cache
often called the \textit{translational look ahead buffer} or TLB, this is for faster translations
the next the same address is accessed. This also brings about the need for coherence between the
entries in the TLB and the page table. The hardware that does all this is called the
\textit{memory management unit} or MMU.\\
Linux OS on x86 (i.e. 32 bit) architecture has 3 levels of the page table. While for x86\_64 (i.e.
64bit) it has 4. They are called \textit{page global directory} or PGD, \textit{page upper
directory} or PUD, \textit{page middle directory} or PMD and \textit{page table entry} or PTE. x86
won't have the PMD. The \textit{CR3} register holds the base address of the PGD.\\

\begin{figure}[tbp]
  \begin{center}
    \includegraphics[width=0.7\textwidth]{images/pagetable}
        	%\caption[Page Table Illustration]{Illustration of \textit{VA} $\rightarrow$ \textit{PA} address translation.}
        	\captionsource{Page Table Illustration}{Illustration of \textit{VA} $\rightarrow$ \textit{PA} address translation.}{\url{corensic.files.wordpress.com/2011/11/virtualmemory.png}}  
    \label{fig:pagetable}
     \end{center}
\end{figure}

\section{Challenges to Virtualizing Memory} \label{challenges}
When we introduce virtualization the physical memory of the system is to be shared among different
VMs. In addition to that there should be ways to dynamically (re)allocate memory among VMs
analogous to the way in which memory is managed among programs in a native system as discussed in
the previous sections. Hence the basic idea of memory virtualization is similar to virtual memory
in native systems. Here on top of the virtual memory abstraction that happens inside a VM we are
introducing another layer of virtual memory sort of abstraction. If we want minimal changes to a
stock guest OS or if the guest OS is unaware of the underlying hypervisor then the guest OS's
perceived view of physical memory should be kept intact by allowing it to maintain the mapping of
virtual addresses to what the guest OS perceives as its physical memory. While in reality what the
guest OS perceives as physical memory is not the real machine memory and neither is it given
unrestricted direct access to the entire real machine memory. This was done to ensure that guest
OSs stay within their bounds and VM isolation was guaranteed. Moreover guest OSs no longer managed
the resources on their own.\\
This was achieved by virtualizing the MMU or in other words by introducing MMU functionalities in
the hypervisor level that backed the the guest OS memory management operations, trying as far as
possible not to break the guest OS's view of the affairs, and that did effective multiplexing of
actual machine memory among the different VMs.\\
Implementing this is not as easy as said. The first challenge lies in deciding how to divide or
partition machine memory among different VMs. Dictating the techniques of access poses another.
Finally the dynamic management aspect comes in.\\
Every VM can be allocated simple, static, contiguous, disjoint fixed partitions or more of a
dynamic allocation scheme similar to demand paging discussed in previous section can be adopted.
The former has obvious drawbacks of memory wastage if the allocated memory is underutilized,
incapability to support a VM requiring more memory than the static allocation etc. All this will
affect the number of VMs that can be hosted on a machine. In addition it cannot support other
virtualization techniques like migration. 
The latter ensures better utilization of memory, enables memory over-commitment among VMs, similar
to the one provided to different processes by normal virtual memory, but introduces considerable
complexity in implementation. One reason for this complexity is the varying levels of dynamism in
memory demands among different VMs which requires the VMM to detect and react quickly to these
demands by adapting its policies for each VM. Another cause of this complexity is multiple memory
resource control entities taking decisions independently; one the guest OS and other the VMM. This
can often result in conflicting decisions being made that increases the memory management
overhead.\\
Since the guest OS employs virtual memory, access to the MMU is often needed. But we also know the
actual MMU functionalities that manages the machine memory lies in the hyervisor. Thus hypervisor
needs to define ways for the guest OS to access these functionalities without breaking VM
isolation.\\
The challenges of implementing memory management at the hypervisor comes next. First, a VMM needs
to accurately determine memory resource usage statistics of individual VM to take any management
policy related decision. Second, policy decisions taken at the VMM level can clash with the
decisions taken within a guest OS. A good example of this the is \textit{double paging} explained
in \citet{waldspurger2002memory}. Third, it is difficult to define a one-copy-fits-all management
strategy for the various types of guest OSs and applications running in different VMs. For e.g. an
application like DB which manages its own memory will be affected badly by a VMM memory management
strategy that transparently reallocates memory assigned to it because for efficient working these
applications need a current, consistent view of the memory allocated to it
\citep{salomie2013application}.\\ 
We now try to look into the existing solutions and implementations of various aspects of memory
virtualization that address many of these issues. 
